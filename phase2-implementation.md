# Phase 2: The "Sensory System" Implementation Plan

## üìã Overview
**Goal:** Build a production-grade data engineering pipeline that ingests, validates, enriches, and stores multi-source data for AI-driven commerce insights.

**Timeline:** 4 weeks  
**Team Size:** 2-3 engineers (1 Data Engineer + 1 Backend + 1 DevOps)  
**Tech Stack:** Kafka/Redpanda, Snowflake, Weaviate, Faust, Playwright

---

## üéØ Success Criteria

| Metric | Target |
|--------|--------|
| Data Pipeline Uptime | >99.5% |
| Scraper Success Rate | >95% |
| Embedding Latency (P95) | <100ms |
| Dead Letter Queue Rate | <2% |
| Schema Validation Pass Rate | >98% |
| End-to-End Latency | <5 seconds |

---

## üèóÔ∏è High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         DATA SOURCES                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Social Media   ‚îÇ   E-Commerce      ‚îÇ   Environmental           ‚îÇ
‚îÇ  (TikTok/FB)    ‚îÇ   (Shopify/Daraz) ‚îÇ   (Weather API)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                 ‚îÇ                      ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
         ‚îî‚îÄ‚ñ∫‚îÇ   Scraper Agents (K8s)      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ   + Pydantic Validation     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ    Apache Kafka (Central Hub)       ‚îÇ
         ‚îÇ  Topics: source.*, sink.*, dlq.*    ‚îÇ
         ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Stream    ‚îÇ              ‚îÇ    Stream       ‚îÇ
    ‚îÇ  Processor  ‚îÇ              ‚îÇ   Processor     ‚îÇ
    ‚îÇ  (Privacy)  ‚îÇ              ‚îÇ  (Enrichment)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                              ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Embedding Service (Faust)    ‚îÇ
         ‚îÇ   Model: all-MiniLM-L6-v2     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                       ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Snowflake     ‚îÇ     ‚îÇ    Weaviate      ‚îÇ
    ‚îÇ (Structured)    ‚îÇ     ‚îÇ   (Vectors)      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÖ Week-by-Week Breakdown

### Week 1: The "Spine" - Kafka Infrastructure

#### Day 1-2: Kafka Cluster Setup

**Deliverables:**
- [ ] Kafka/Redpanda deployed (Docker Compose for dev, K8s for prod)
- [ ] Zookeeper configured (if using Kafka)
- [ ] Basic cluster health monitoring

**Commands:**
```bash
# Create project directory
mkdir millionx-phase2
cd millionx-phase2

# Create Docker Compose for local dev
cat > docker-compose.kafka.yml
```

**docker-compose.kafka.yml:**
```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: millionx-zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: millionx-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
    volumes:
      - kafka_data:/var/lib/kafka/data

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: millionx-kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: millionx-local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:

networks:
  default:
    name: millionx-network
```

**Validation:**
```bash
# Start Kafka stack
docker-compose -f docker-compose.kafka.yml up -d

# Check health
docker ps | grep millionx

# Access Kafka UI
# Open browser: http://localhost:8080
```

#### Day 3-4: Topic Creation & Configuration

**Create topics script:**
```bash
# kafka-topics.sh
#!/bin/bash

KAFKA_CONTAINER="millionx-kafka"

# Source topics (Raw ingestion)
docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.social.tiktok \
  --partitions 6 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.social.facebook \
  --partitions 6 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.market.shopify \
  --partitions 4 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.market.daraz \
  --partitions 4 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic context.weather \
  --partitions 2 \
  --replication-factor 1 \
  --config retention.ms=2592000000 \
  --bootstrap-server localhost:9092

# Sink topics (Processed data)
docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic sink.snowflake.orders \
  --partitions 4 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic sink.weaviate.vectors \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Dead Letter Queues
docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic dead-letters-social \
  --partitions 2 \
  --replication-factor 1 \
  --config retention.ms=1209600000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic dead-letters-market \
  --partitions 2 \
  --replication-factor 1 \
  --config retention.ms=1209600000 \
  --bootstrap-server localhost:9092

echo "‚úÖ All topics created successfully!"
```

**Validation:**
```bash
chmod +x kafka-topics.sh
./kafka-topics.sh

# List all topics
docker exec millionx-kafka kafka-topics --list --bootstrap-server localhost:9092
```

#### Day 5: Monitoring Setup

**Install Prometheus + Grafana:**
```yaml
# Add to docker-compose.kafka.yml

  prometheus:
    image: prom/prometheus:latest
    container_name: millionx-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana:latest
    container_name: millionx-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  prometheus_data:
  grafana_data:
```

**prometheus.yml:**
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka:9101']
```

---

### Week 2: The "Eyes" - Scraper Agents

#### Day 1-3: Social Media Scrapers

**Project Structure:**
```
scrapers/
‚îú‚îÄ‚îÄ social/
‚îÇ   ‚îú‚îÄ‚îÄ tiktok_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ facebook_scraper.py
‚îÇ   ‚îî‚îÄ‚îÄ models.py
‚îú‚îÄ‚îÄ market/
‚îÇ   ‚îú‚îÄ‚îÄ shopify_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ daraz_integration.py
‚îú‚îÄ‚îÄ shared/
‚îÇ   ‚îú‚îÄ‚îÄ kafka_producer.py
‚îÇ   ‚îú‚îÄ‚îÄ dlq_handler.py
‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ Dockerfile
```

**models.py (Pydantic Schemas):**
```python
from pydantic import BaseModel, Field, validator
from datetime import datetime
from typing import Optional, List

class SocialPost(BaseModel):
    """Strict schema for social media posts"""
    platform: str = Field(..., regex="^(tiktok|facebook)$")
    post_id: str
    author_username: str  # Will be hashed by privacy shield
    content: str
    engagement_count: int = Field(ge=0)
    timestamp: datetime
    hashtags: List[str] = []
    product_mentions: Optional[List[str]] = None
    
    @validator('content')
    def content_not_empty(cls, v):
        if not v or len(v.strip()) < 5:
            raise ValueError('Content must be at least 5 characters')
        return v

class MarketOrder(BaseModel):
    """Schema for e-commerce orders"""
    platform: str = Field(..., regex="^(shopify|daraz)$")
    order_id: str
    merchant_id: str
    product_id: str
    product_name: str
    quantity: int = Field(gt=0)
    price: float = Field(gt=0)
    currency: str = Field(default="BDT")
    order_status: str
    customer_phone: Optional[str] = None  # Will be hashed
    timestamp: datetime

class WeatherData(BaseModel):
    """Schema for weather updates"""
    city: str
    temperature: float
    condition: str  # "clear", "rain", "storm", etc.
    humidity: float = Field(ge=0, le=100)
    wind_speed: float
    timestamp: datetime
    is_extreme_weather: bool = False
```

**tiktok_scraper.py:**
```python
import asyncio
from playwright.async_api import async_playwright
from kafka_producer import KafkaProducerClient
from models import SocialPost
from dlq_handler import send_to_dlq
import logging
from datetime import datetime
import random
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rotating proxy pool (use BrightData, Smartproxy, or Oxylabs in production)
PROXY_LIST = os.getenv('PROXY_LIST', '').split(',')  # Format: http://user:pass@host:port
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

class TikTokScraper:
    def __init__(self, keywords: List[str]):
        self.keywords = keywords
        self.producer = KafkaProducerClient(topic="source.social.tiktok")
        
    async def scrape(self):
        """Scrape TikTok for trending Benglish commerce keywords"""
        async with async_playwright() as p:
            # Configure browser with rotating proxy and user agent
            proxy_config = None
            if PROXY_LIST and PROXY_LIST[0]:
                proxy_config = {"server": random.choice(PROXY_LIST)}
            
            browser = await p.chromium.launch(
                headless=True,
                proxy=proxy_config,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            # Random user agent to avoid fingerprinting
            context = await browser.new_context(
                user_agent=random.choice(USER_AGENTS),
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            for keyword in self.keywords:
                try:
                    # Navigate to TikTok search
                    url = f"https://www.tiktok.com/search?q={keyword}"
                    await page.goto(url, wait_until="networkidle")
                    
                    # Wait for posts to load
                    await page.wait_for_selector('[data-e2e="search-result"]', timeout=10000)
                    
                    # Extract posts
                    posts = await page.query_selector_all('[data-e2e="search-result"]')
                    
                    for post in posts[:20]:  # Limit to top 20
                        try:
                            # Extract data
                            author = await post.query_selector('[data-e2e="search-user-name"]')
                            content = await post.query_selector('[data-e2e="search-desc"]')
                            likes = await post.query_selector('[data-e2e="like-count"]')
                            
                            if not (author and content):
                                continue
                            
                            author_text = await author.inner_text()
                            content_text = await content.inner_text()
                            likes_text = await likes.inner_text() if likes else "0"
                            
                            # Validate with Pydantic
                            social_post = SocialPost(
                                platform="tiktok",
                                post_id=f"tiktok_{datetime.now().timestamp()}_{hash(content_text)}",
                                author_username=author_text,
                                content=content_text,
                                engagement_count=self._parse_count(likes_text),
                                timestamp=datetime.now(),
                                hashtags=[keyword],
                                product_mentions=self._extract_products(content_text)
                            )
                            
                            # Send to Kafka
                            await self.producer.send(social_post.dict())
                            logger.info(f"‚úÖ Scraped TikTok post: {social_post.post_id}")
                            
                        except Exception as e:
                            # Send to DLQ if validation fails
                            logger.error(f"‚ùå Failed to process post: {e}")
                            await send_to_dlq(
                                topic="dead-letters-social",
                                data={"error": str(e), "raw": await post.inner_html()}
                            )
                    
                    # Rate limiting
                    await asyncio.sleep(5)
                    
                except Exception as e:
                    logger.error(f"‚ùå Failed to scrape keyword '{keyword}': {e}")
            
            await browser.close()
    
    def _parse_count(self, text: str) -> int:
        """Parse TikTok count format (e.g., '1.2K' -> 1200)"""
        text = text.strip().upper()
        multipliers = {'K': 1000, 'M': 1000000}
        
        for suffix, mult in multipliers.items():
            if suffix in text:
                return int(float(text.replace(suffix, '')) * mult)
        return int(text) if text.isdigit() else 0
    
    def _extract_products(self, text: str) -> List[str]:
        """Extract product mentions from text"""
        # Simple keyword matching (improve with NER later)
        products = []
        product_keywords = ['bag', 'shoes', 'dress', 'phone', 'watch', 'juta', 'shari']
        
        for keyword in product_keywords:
            if keyword.lower() in text.lower():
                products.append(keyword)
        
        return products

# Run as Kubernetes CronJob
if __name__ == "__main__":
    keywords = [
        "online shopping bangladesh",
        "daraz deal",
        "facebook shop",
        "delivery dhaka",
        "trending product"
    ]
    
    scraper = TikTokScraper(keywords)
    asyncio.run(scraper.scrape())
```

**kafka_producer.py:**
```python
from kafka import KafkaProducer
import json
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class KafkaProducerClient:
    def __init__(self, topic: str, bootstrap_servers: str = "localhost:9092"):
        self.topic = topic
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',  # Wait for all replicas
            retries=3,
            max_in_flight_requests_per_connection=1  # Ensure ordering
        )
    
    async def send(self, data: Dict[str, Any]):
        """Send message to Kafka topic"""
        try:
            future = self.producer.send(self.topic, value=data)
            record_metadata = future.get(timeout=10)
            logger.info(f"‚úÖ Sent to {self.topic}: partition {record_metadata.partition}, offset {record_metadata.offset}")
        except Exception as e:
            logger.error(f"‚ùå Failed to send to Kafka: {e}")
            raise
    
    def close(self):
        self.producer.flush()
        self.producer.close()
```

**dlq_handler.py:**
```python
from kafka_producer import KafkaProducerClient
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

async def send_to_dlq(topic: str, data: dict, error: str = None):
    """Send failed message to Dead Letter Queue"""
    dlq_producer = KafkaProducerClient(topic=topic)
    
    dlq_message = {
        "original_data": data,
        "error": error or "Unknown error",
        "timestamp": datetime.now().isoformat(),
        "retry_count": 0
    }
    
    try:
        await dlq_producer.send(dlq_message)
        logger.info(f"üìÆ Sent to DLQ: {topic}")
    except Exception as e:
        logger.error(f"‚ùå Failed to send to DLQ: {e}")
    finally:
        dlq_producer.close()
```

**requirements.txt:**
```
kafka-python==2.0.2
playwright==1.40.0
pydantic==2.5.0
python-dotenv==1.0.0
requests==2.31.0
proxy-requests==0.6.0  # For rotating proxies
python-anticaptcha==1.0.0  # CAPTCHA solving (optional)
apify-client==1.3.0  # Fallback to Apify actors (optional)
snowflake-connector-python[pandas]==3.5.0  # For batch loading
pandas==2.1.4  # Required for write_pandas
```

**Dockerfile:**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies for Playwright
RUN apt-get update && apt-get install -y \
    wget \
    ca-certificates \
    fonts-liberation \
    libasound2 \
    libatk-bridge2.0-0 \
    libatk1.0-0 \
    libc6 \
    libcairo2 \
    libcups2 \
    libdbus-1-3 \
    libexpat1 \
    libfontconfig1 \
    libgbm1 \
    libgcc1 \
    libglib2.0-0 \
    libgtk-3-0 \
    libnspr4 \
    libnss3 \
    libpango-1.0-0 \
    libpangocairo-1.0-0 \
    libstdc++6 \
    libx11-6 \
    libx11-xcb1 \
    libxcb1 \
    libxcomposite1 \
    libxcursor1 \
    libxdamage1 \
    libxext6 \
    libxfixes3 \
    libxi6 \
    libxrandr2 \
    libxrender1 \
    libxss1 \
    libxtst6 \
    lsb-release \
    xdg-utils \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install Playwright browsers
RUN playwright install chromium

COPY . .

CMD ["python", "social/tiktok_scraper.py"]
```

#### Day 4-5: Market Integrations

**shopify_integration.py:**
```python
import shopify
from kafka_producer import KafkaProducerClient
from models import MarketOrder
from datetime import datetime
import logging
import os

logger = logging.getLogger(__name__)

class ShopifyIntegration:
    def __init__(self):
        # Setup Shopify API
        shop_url = os.getenv("SHOPIFY_SHOP_URL")
        api_version = os.getenv("SHOPIFY_API_VERSION", "2024-01")
        access_token = os.getenv("SHOPIFY_ACCESS_TOKEN")
        
        shopify.ShopifyResource.set_site(f"https://{shop_url}/admin/api/{api_version}")
        shopify.ShopifyResource.activate_session(shopify.Session(shop_url, api_version, access_token))
        
        self.producer = KafkaProducerClient(topic="source.market.shopify")
    
    async def sync_orders(self, since_id: int = None):
        """Sync orders from Shopify"""
        try:
            # Fetch orders
            params = {"status": "any", "limit": 250}
            if since_id:
                params["since_id"] = since_id
            
            orders = shopify.Order.find(**params)
            
            for order in orders:
                try:
                    for line_item in order.line_items:
                        # Validate with Pydantic
                        market_order = MarketOrder(
                            platform="shopify",
                            order_id=str(order.id),
                            merchant_id=os.getenv("MERCHANT_ID"),
                            product_id=str(line_item.product_id),
                            product_name=line_item.name,
                            quantity=line_item.quantity,
                            price=float(line_item.price),
                            currency=order.currency,
                            order_status=order.financial_status,
                            customer_phone=order.customer.phone if order.customer else None,
                            timestamp=datetime.fromisoformat(order.created_at)
                        )
                        
                        # Send to Kafka
                        await self.producer.send(market_order.dict())
                        logger.info(f"‚úÖ Synced Shopify order: {order.id}")
                
                except Exception as e:
                    logger.error(f"‚ùå Failed to process order {order.id}: {e}")
                    await send_to_dlq("dead-letters-market", order.to_dict(), str(e))
        
        except Exception as e:
            logger.error(f"‚ùå Shopify sync failed: {e}")
    
    def setup_webhook(self):
        """Setup webhook for real-time order updates"""
        webhook = shopify.Webhook()
        webhook.topic = "orders/create"
        webhook.address = os.getenv("WEBHOOK_URL") + "/shopify/orders"
        webhook.format = "json"
        
        if webhook.save():
            logger.info("‚úÖ Shopify webhook created")
        else:
            logger.error(f"‚ùå Webhook creation failed: {webhook.errors.full_messages()}")

if __name__ == "__main__":
    integration = ShopifyIntegration()
    asyncio.run(integration.sync_orders())
```

#### ‚ö†Ô∏è Risk Mitigation: The "Scraping War"

**Problem:** TikTok/Facebook have aggressive anti-bot measures that will block simple scrapers within minutes.

**Solutions (Choose based on budget):**

1. **Phase 2a - Enhanced Scraping (Lower Cost):**
   - Use rotating residential proxies (BrightData, Smartproxy, Oxylabs)
   - Implement CAPTCHA solving service (2Captcha, Anti-Captcha)
   - Add random delays between requests (3-8 seconds)
   - Rotate user agents and browser fingerprints
   - Cost: ~$500-1000/month for proxies

2. **Phase 2b - API Services (Higher Reliability):**
   - Use Apify actors for TikTok/Facebook data
   - Alternative: RapidAPI social media endpoints
   - Benefit: Zero maintenance, built-in rate limiting
   - Cost: ~$200-500/month depending on volume
   - **Recommended for production to reduce engineering overhead**

**Implementation:**
```bash
# Add to requirements.txt
proxy-requests==0.6.0
python-anticaptcha==1.0.0

# Environment variables
export PROXY_LIST="http://user:pass@proxy1.com:8080,http://user:pass@proxy2.com:8080"
export APIFY_TOKEN="your_apify_token"  # If using Apify fallback
```

---

### Week 3: The "Brain" - Stream Processing

#### Day 1-2: Privacy Shield (PII Anonymization)

**privacy_shield.py:**
```python
import faust
import hashlib
import re
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

app = faust.App(
    'privacy-shield',
    broker='kafka://localhost:9092',
    value_serializer='json'
)

# Input topics
social_topic = app.topic('source.social.tiktok', 'source.social.facebook')
market_topic = app.topic('source.market.shopify', 'source.market.daraz')

# Output topics
anonymized_social = app.topic('sink.weaviate.vectors')
anonymized_market = app.topic('sink.snowflake.orders')

# Regex patterns for PII
PHONE_PATTERN = re.compile(r'\+?880[0-9]{10}|\b0[0-9]{10}\b')
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
NAME_PATTERN = re.compile(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b')  # Simple heuristic

SALT = "millionx_privacy_salt_2025"  # Move to secrets manager in prod

def hash_pii(text: str) -> str:
    """Hash PII with SHA-256"""
    return hashlib.sha256((text + SALT).encode()).hexdigest()[:16]

def anonymize_text(text: str) -> str:
    """Replace PII in text with hashed versions"""
    # Anonymize phone numbers
    text = PHONE_PATTERN.sub(lambda m: f"PHONE_{hash_pii(m.group())}", text)
    
    # Anonymize emails
    text = EMAIL_PATTERN.sub(lambda m: f"EMAIL_{hash_pii(m.group())}", text)
    
    # Anonymize potential names (be conservative)
    # text = NAME_PATTERN.sub(lambda m: f"USER_{hash_pii(m.group())}", text)
    
    return text

@app.agent(social_topic)
async def process_social(stream):
    """Anonymize social media posts"""
    async for message in stream:
        try:
            # Anonymize username
            if 'author_username' in message:
                message['author_username_hash'] = hash_pii(message['author_username'])
                del message['author_username']
            
            # Anonymize content
            if 'content' in message:
                message['content'] = anonymize_text(message['content'])
            
            # Forward to next stage
            await anonymized_social.send(value=message)
            logger.info(f"‚úÖ Anonymized social post: {message.get('post_id')}")
        
        except Exception as e:
            logger.error(f"‚ùå Privacy shield failed: {e}")

@app.agent(market_topic)
async def process_market(stream):
    """Anonymize market orders"""
    async for message in stream:
        try:
            # Hash customer phone
            if message.get('customer_phone'):
                message['customer_phone_hash'] = hash_pii(message['customer_phone'])
                del message['customer_phone']
            
            # Forward to Snowflake
            await anonymized_market.send(value=message)
            logger.info(f"‚úÖ Anonymized order: {message.get('order_id')}")
        
        except Exception as e:
            logger.error(f"‚ùå Privacy shield failed: {e}")

if __name__ == '__main__':
    app.main()
```

**Run:**
```bash
# Install Faust
pip install faust-streaming

# Start privacy shield worker
faust -A privacy_shield worker -l info
```

#### Day 3-4: Context Enrichment

**context_enricher.py:**
```python
import faust
import redis
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

app = faust.App(
    'context-enricher',
    broker='kafka://localhost:9092',
    value_serializer='json'
)

# Redis for product metadata cache
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

input_topic = app.topic('sink.weaviate.vectors')
output_topic = app.topic('enriched.weaviate.vectors')

@app.agent(input_topic)
async def enrich_social(stream):
    """Enrich social posts with product context"""
    async for message in stream:
        try:
            product_mentions = message.get('product_mentions', [])
            
            if product_mentions:
                # Look up product metadata from Redis cache
                enriched_context = []
                
                for product_id in product_mentions:
                    product_data = redis_client.get(f"product:{product_id}")
                    
                    if product_data:
                        enriched_context.append(product_data)
                    else:
                        logger.warning(f"‚ö†Ô∏è Product {product_id} not found in cache")
                
                # Create composite text for embedding
                message['enriched_content'] = f"{message.get('content', '')} [Products: {', '.join(enriched_context)}]"
            else:
                message['enriched_content'] = message.get('content', '')
            
            await output_topic.send(value=message)
            logger.info(f"‚úÖ Enriched post: {message.get('post_id')}")
        
        except Exception as e:
            logger.error(f"‚ùå Enrichment failed: {e}")

if __name__ == '__main__':
    app.main()
```

#### Day 5: Embedding Service

**embedding_service.py:**
```python
import faust
from sentence_transformers import SentenceTransformer
import weaviate
from typing import Dict, Any
import logging
import numpy as np

logger = logging.getLogger(__name__)

app = faust.App(
    'embedding-service',
    broker='kafka://localhost:9092',
    value_serializer='json'
)

# Load embedding model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Connect to Weaviate
weaviate_client = weaviate.Client("http://localhost:8082")

input_topic = app.topic('enriched.weaviate.vectors')

@app.agent(input_topic)
async def embed_and_store(stream):
    """Generate embeddings and store in Weaviate"""
    async for message in stream:
        try:
            # Get text to embed
            text = message.get('enriched_content', message.get('content', ''))
            
            if not text:
                logger.warning("‚ö†Ô∏è No content to embed")
                continue
            
            # Generate embedding
            vector = model.encode(text).tolist()
            
            # Prepare Weaviate object
            data_object = {
                "platform": message.get('platform'),
                "content": text,
                "timestamp": message.get('timestamp'),
                "engagement": message.get('engagement_count', 0),
                "post_id": message.get('post_id'),
                "hashtags": message.get('hashtags', [])
            }
            
            # Store in Weaviate
            result = weaviate_client.data_object.create(
                data_object=data_object,
                class_name="SocialPost",
                vector=vector
            )
            
            logger.info(f"‚úÖ Stored in Weaviate: {result}")
        
        except Exception as e:
            logger.error(f"‚ùå Embedding failed: {e}")

if __name__ == '__main__':
    app.main()
```

---

### Week 4: Storage & Monitoring

#### Day 1-2: Snowflake Integration

**‚ö†Ô∏è COST OPTIMIZATION: Use Kafka Connect (Recommended)**

The row-by-row INSERT approach is inefficient and expensive in Snowflake. Instead, use the official Kafka Connect Snowflake Sink Connector for automatic batching.

**kafka-connect-snowflake.json:**
```json
{
  "name": "snowflake-sink-orders",
  "config": {
    "connector.class": "com.snowflake.kafka.connector.SnowflakeSinkConnector",
    "tasks.max": "4",
    "topics": "sink.snowflake.orders",
    "snowflake.url.name": "${SNOWFLAKE_ACCOUNT}.snowflakecomputing.com",
    "snowflake.user.name": "${SNOWFLAKE_USER}",
    "snowflake.private.key": "${SNOWFLAKE_PRIVATE_KEY}",
    "snowflake.database.name": "MILLIONX",
    "snowflake.schema.name": "RAW_DATA",
    "buffer.count.records": "10000",
    "buffer.flush.time": "60",
    "buffer.size.bytes": "5000000",
    "snowflake.ingestion.method": "SNOWPIPE_STREAMING",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false"
  }
}
```

**Deploy Kafka Connect:**
```yaml
# Add to docker-compose.kafka.yml
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    hostname: kafka-connect
    container_name: millionx-kafka-connect
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: millionx-connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components'
    volumes:
      - ./kafka-connect-plugins:/usr/share/confluent-hub-components
```

**Install Snowflake Connector:**
```bash
# Download connector
confluent-hub install snowflakeinc/snowflake-kafka-connector:2.0.0

# Deploy connector config
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @kafka-connect-snowflake.json

# Verify status
curl http://localhost:8083/connectors/snowflake-sink-orders/status
```

**Alternative: Batch Loading with Pandas (If Kafka Connect is not feasible)**

```python
import pandas as pd
import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas
from kafka import KafkaConsumer
import logging
import os
import json

logger = logging.getLogger(__name__)

class SnowflakeBatchSink:
    def __init__(self, batch_size=1000, flush_interval=60):
        self.conn = snowflake.connector.connect(
            user=os.getenv('SNOWFLAKE_USER'),
            password=os.getenv('SNOWFLAKE_PASSWORD'),
            account=os.getenv('SNOWFLAKE_ACCOUNT'),
            warehouse='COMPUTE_WH',
            database='MILLIONX',
            schema='RAW_DATA'
        )
        
        self.consumer = KafkaConsumer(
            'sink.snowflake.orders',
            bootstrap_servers='localhost:9092',
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id='snowflake-batch-sink',
            enable_auto_commit=False
        )
        
        self.batch_size = batch_size
        self.buffer = []
    
    def consume_and_batch_load(self):
        """Batch load to Snowflake using Pandas"""
        for message in self.consumer:
            self.buffer.append(message.value)
            
            if len(self.buffer) >= self.batch_size:
                self._flush_to_snowflake()
    
    def _flush_to_snowflake(self):
        """Write batch using Pandas (much faster than row-by-row)"""
        if not self.buffer:
            return
        
        try:
            df = pd.DataFrame(self.buffer)
            success, nchunks, nrows, _ = write_pandas(
                self.conn,
                df,
                'ORDERS',
                auto_create_table=False,
                quote_identifiers=False
            )
            
            if success:
                logger.info(f"‚úÖ Loaded {nrows} rows to Snowflake in {nchunks} chunks")
                self.consumer.commit()
                self.buffer.clear()
            else:
                logger.error("‚ùå Batch load failed")
        
        except Exception as e:
            logger.error(f"‚ùå Snowflake batch load error: {e}")

if __name__ == '__main__':
    sink = SnowflakeBatchSink(batch_size=1000)
    sink.consume_and_batch_load()
```

**Cost Comparison:**
- ‚ùå **Row-by-row INSERT:** ~$50-100/day for 1M records (lots of micro-transactions)
- ‚úÖ **Kafka Connect Streaming:** ~$5-10/day (optimized batching)
- ‚úÖ **Pandas write_pandas:** ~$8-15/day (good middle ground)

**Recommendation:** Use Kafka Connect for production. It handles retries, schema evolution, and monitoring automatically.

**Snowflake Schema:**
```sql
-- Create database
CREATE DATABASE MILLIONX;

-- Create schema
CREATE SCHEMA RAW_DATA;

-- Orders table
CREATE TABLE orders (
    order_id VARCHAR(100) PRIMARY KEY,
    platform VARCHAR(50),
    merchant_id VARCHAR(100),
    product_id VARCHAR(100),
    product_name VARCHAR(500),
    quantity INTEGER,
    price FLOAT,
    currency VARCHAR(10),
    order_status VARCHAR(50),
    customer_phone_hash VARCHAR(64),
    timestamp TIMESTAMP,
    ingestion_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Create indexes
CREATE INDEX idx_merchant ON orders(merchant_id);
CREATE INDEX idx_timestamp ON orders(timestamp);
CREATE INDEX idx_product ON orders(product_id);

-- Price history table
CREATE TABLE price_history (
    id INTEGER AUTOINCREMENT PRIMARY KEY,
    product_id VARCHAR(100),
    platform VARCHAR(50),
    price FLOAT,
    recorded_at TIMESTAMP
);

-- Weather logs table
CREATE TABLE weather_logs (
    id INTEGER AUTOINCREMENT PRIMARY KEY,
    city VARCHAR(100),
    temperature FLOAT,
    condition VARCHAR(100),
    humidity FLOAT,
    wind_speed FLOAT,
    is_extreme_weather BOOLEAN,
    timestamp TIMESTAMP
);
```

#### Day 3: Weaviate Setup

**weaviate_schema.py:**
```python
import weaviate

client = weaviate.Client("http://localhost:8082")

# Define schema
schema = {
    "classes": [
        {
            "class": "SocialPost",
            "description": "Social media posts with semantic embeddings",
            "vectorizer": "none",  # We provide vectors manually
            "properties": [
                {
                    "name": "platform",
                    "dataType": ["string"],
                    "description": "Social media platform (tiktok/facebook)"
                },
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "Post content"
                },
                {
                    "name": "timestamp",
                    "dataType": ["date"],
                    "description": "Post timestamp"
                },
                {
                    "name": "engagement",
                    "dataType": ["int"],
                    "description": "Total engagement count"
                },
                {
                    "name": "post_id",
                    "dataType": ["string"],
                    "description": "Unique post identifier"
                },
                {
                    "name": "hashtags",
                    "dataType": ["string[]"],
                    "description": "Associated hashtags"
                }
            ]
        }
    ]
}

# Create schema
client.schema.create(schema)
print("‚úÖ Weaviate schema created")
```

**Deploy Weaviate:**
```yaml
# Add to docker-compose
  weaviate:
    image: semitechnologies/weaviate:1.23.0
    container_name: millionx-weaviate
    ports:
      - "8082:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate_data:/var/lib/weaviate
```

#### Day 4: Weather Integration

**weather_fetcher.py:**
```python
import requests
from kafka_producer import KafkaProducerClient
from models import WeatherData
from datetime import datetime
import logging
import os

logger = logging.getLogger(__name__)

class WeatherFetcher:
    def __init__(self):
        self.api_key = os.getenv("OPENWEATHER_API_KEY")
        self.cities = ["Dhaka", "Chittagong", "Sylhet", "Khulna", "Rajshahi"]
        self.producer = KafkaProducerClient(topic="context.weather")
    
    async def fetch_weather(self):
        """Fetch weather for all major cities"""
        for city in self.cities:
            try:
                url = f"https://api.openweathermap.org/data/2.5/weather?q={city},BD&appid={self.api_key}&units=metric"
                response = requests.get(url)
                response.raise_for_status()
                
                data = response.json()
                
                # Check for extreme weather
                is_extreme = (
                    data['main']['temp'] > 40 or
                    data['main']['temp'] < 5 or
                    data['wind']['speed'] > 15 or
                    'rain' in data.get('weather', [{}])[0].get('main', '').lower()
                )
                
                weather = WeatherData(
                    city=city,
                    temperature=data['main']['temp'],
                    condition=data['weather'][0]['main'],
                    humidity=data['main']['humidity'],
                    wind_speed=data['wind']['speed'],
                    timestamp=datetime.now(),
                    is_extreme_weather=is_extreme
                )
                
                await self.producer.send(weather.dict())
                logger.info(f"‚úÖ Fetched weather for {city}: {weather.condition}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to fetch weather for {city}: {e}")

# Setup as Kubernetes CronJob (every hour)
if __name__ == "__main__":
    import asyncio
    fetcher = WeatherFetcher()
    asyncio.run(fetcher.fetch_weather())
```

#### Day 5: Monitoring Dashboard

**Key Metrics to Track:**
1. **Kafka Lag:** Consumer group lag per topic
2. **Throughput:** Messages/sec ingested
3. **DLQ Rate:** Failed messages %
4. **Scraper Success Rate:** Successful scrapes %
5. **Embedding Latency:** P95/P99 latency
6. **Storage Write Rate:** Snowflake/Weaviate inserts/sec

**Grafana Dashboard JSON:**
```json
{
  "dashboard": {
    "title": "MillionX Phase 2 - Data Pipeline",
    "panels": [
      {
        "title": "Kafka Consumer Lag",
        "targets": [
          {
            "expr": "kafka_consumer_lag{topic=~'source.*'}",
            "legendFormat": "{{topic}}"
          }
        ]
      },
      {
        "title": "DLQ Message Rate",
        "targets": [
          {
            "expr": "rate(kafka_topic_messages_total{topic=~'dead-letters.*'}[5m])",
            "legendFormat": "{{topic}}"
          }
        ]
      }
    ]
  }
}
```

---

## üß™ Testing Checklist

### Unit Tests
- [ ] Pydantic model validation (valid/invalid data)
- [ ] PII anonymization function (phone/email detection)
- [ ] Embedding generation (verify vector dimensions)
- [ ] DLQ handler (ensure messages reach DLQ)

### Integration Tests
- [ ] End-to-end flow: Scraper ‚Üí Kafka ‚Üí Weaviate
- [ ] Privacy shield: PII removed before storage
- [ ] Context enricher: Product metadata attached
- [ ] Snowflake sink: Data lands in correct tables

### Performance Tests
- [ ] Kafka throughput: >10,000 msg/sec
- [ ] Embedding latency: P95 < 100ms
- [ ] Scraper completion: <5 minutes for 100 posts
- [ ] DLQ rate: <2% of total messages

---

## üöÄ Deployment Guide

### Kubernetes CronJobs

**k8s/tiktok-scraper-cronjob.yaml:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: tiktok-scraper
spec:
  schedule: "0 * * * *"  # Every hour
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: scraper
            image: millionx/tiktok-scraper:latest
            env:
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "kafka:9092"
          restartPolicy: OnFailure
```

### Faust Workers (Kubernetes Deployment)

**k8s/privacy-shield-deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: privacy-shield
spec:
  replicas: 3
  selector:
    matchLabels:
      app: privacy-shield
  template:
    metadata:
      labels:
        app: privacy-shield
    spec:
      containers:
      - name: worker
        image: millionx/privacy-shield:latest
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka:9092"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

---

## üìä Success Validation

### Week 1 ‚úÖ
- Kafka cluster running
- All topics created
- Monitoring dashboards live

### Week 2 ‚úÖ
- Scrapers fetching 100+ posts/hour
- DLQ capturing invalid messages
- Market integrations syncing orders

### Week 3 ‚úÖ
- PII anonymized at source
- Context enrichment working
- Embeddings generated (<100ms P95)

### Week 4 ‚úÖ
- Snowflake receiving structured data
- Weaviate storing vectors
- Weather data streaming hourly
- All metrics within targets

---

## üîÑ Next Steps ‚Üí Phase 3

Once Phase 2 is validated, proceed to:
1. **AI Model Training:** Use Snowflake data for NeuralProphet demand forecasting
2. **LangGraph Integration:** Connect agents to data sources
3. **Real-time Analytics:** Build dashboards for merchants
4. **Predictive Alerts:** Notify merchants of trending products

**Estimated Start Date:** Week 5  
**Prerequisites:** Phase 2 pipeline running at >95% uptime for 1 week

---

## ‚ö†Ô∏è Critical Production Considerations

### Risk A: Anti-Bot Defenses (Already Addressed)
‚úÖ **Status:** Mitigated in tiktok_scraper.py
- Rotating proxy support added (BrightData/Smartproxy compatible)
- User agent rotation implemented
- CAPTCHA solving integration ready (python-anticaptcha)
- **Fallback Option:** Apify/RapidAPI actors for zero-maintenance scraping

### Risk B: Snowflake Cost Optimization (Already Addressed)  
‚úÖ **Status:** Mitigated with Kafka Connect approach
- **Primary:** Kafka Connect Snowflake Sink (automated batching, ~$5-10/day)
- **Alternative:** Pandas write_pandas batch loader (~$8-15/day)
- ‚ùå **Avoid:** Row-by-row INSERT approach (~$50-100/day) - removed from implementation

**Cost Savings:** ~80-90% reduction in Snowflake ingestion costs compared to naive implementation.

### Additional Production Hardening
- [ ] Implement circuit breakers for external API failures
- [ ] Add distributed tracing (OpenTelemetry) for end-to-end observability
- [ ] Setup PagerDuty/OpsGenie alerts for DLQ threshold breaches
- [ ] Implement data quality SLAs with Great Expectations framework
- [ ] Add backup/disaster recovery for Kafka topics (MirrorMaker 2)

---

**Document Version:** 1.1  
**Last Updated:** December 20, 2025  
**Change Log:** Added real-world risk mitigations (proxy rotation, Snowflake optimization)  
**Next Review:** After Week 2 completion
