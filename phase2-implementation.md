# Phase 2: The "Sensory System" Implementation Plan

## ðŸ“‹ Overview
**Goal:** Build a production-grade data engineering pipeline that ingests, validates, enriches, and stores multi-source data for AI-driven commerce insights.

**Timeline:** 4 weeks  
**Team Size:** 2-3 engineers (1 Data Engineer + 1 Backend + 1 DevOps)  
**Tech Stack:** Kafka/Redpanda, Snowflake, Weaviate, Faust, Playwright

---

## ðŸŽ¯ Success Criteria

| Metric | Target |
|--------|--------|
| Data Pipeline Uptime | >99.5% |
| Scraper Success Rate | >95% |
| Embedding Latency (P95) | <100ms |
| Dead Letter Queue Rate | <2% |
| Schema Validation Pass Rate | >98% |
| End-to-End Latency | <5 seconds |

---

## ðŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Social Media   â”‚   E-Commerce      â”‚   Environmental           â”‚
â”‚  (TikTok/FB)    â”‚   (Shopify/Daraz) â”‚   (Weather API)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                      â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
         â””â”€â–ºâ”‚   Scraper Agents (K8s)      â”‚â—„â”€â”€â”€â”€â”€â”˜
            â”‚   + Pydantic Validation     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Apache Kafka (Central Hub)       â”‚
         â”‚  Topics: source.*, sink.*, dlq.*    â”‚
         â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Stream    â”‚              â”‚    Stream       â”‚
    â”‚  Processor  â”‚              â”‚   Processor     â”‚
    â”‚  (Privacy)  â”‚              â”‚  (Enrichment)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Embedding Service (Faust)    â”‚
         â”‚   Model: all-MiniLM-L6-v2     â”‚
         â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
             â”‚                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Snowflake     â”‚     â”‚    Weaviate      â”‚
    â”‚ (Structured)    â”‚     â”‚   (Vectors)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“… Week-by-Week Breakdown

### Week 1: The "Spine" - Kafka Infrastructure

#### Day 1-2: Kafka Cluster Setup

**Deliverables:**
- [ ] Kafka/Redpanda deployed (Docker Compose for dev, K8s for prod)
- [ ] Zookeeper configured (if using Kafka)
- [ ] Basic cluster health monitoring

**Commands:**
```bash
# Create project directory
mkdir millionx-phase2
cd millionx-phase2

# Create Docker Compose for local dev
cat > docker-compose.kafka.yml
```

**docker-compose.kafka.yml:**
```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: millionx-zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: millionx-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
    volumes:
      - kafka_data:/var/lib/kafka/data

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: millionx-kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: millionx-local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:

networks:
  default:
    name: millionx-network
```

**Validation:**
```bash
# Start Kafka stack
docker-compose -f docker-compose.kafka.yml up -d

# Check health
docker ps | grep millionx

# Access Kafka UI
# Open browser: http://localhost:8080
```

#### Day 3-4: Topic Creation & Configuration

**Create topics script:**
```bash
# kafka-topics.sh
#!/bin/bash

KAFKA_CONTAINER="millionx-kafka"

# Source topics (Raw ingestion)
docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.social.tiktok \
  --partitions 6 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.social.facebook \
  --partitions 6 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.market.shopify \
  --partitions 4 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic source.market.daraz \
  --partitions 4 \
  --replication-factor 1 \
  --config retention.ms=604800000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic context.weather \
  --partitions 2 \
  --replication-factor 1 \
  --config retention.ms=2592000000 \
  --bootstrap-server localhost:9092

# Sink topics (Processed data)
docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic sink.snowflake.orders \
  --partitions 4 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic sink.weaviate.vectors \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Dead Letter Queues
docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic dead-letters-social \
  --partitions 2 \
  --replication-factor 1 \
  --config retention.ms=1209600000 \
  --bootstrap-server localhost:9092

docker exec $KAFKA_CONTAINER kafka-topics --create \
  --topic dead-letters-market \
  --partitions 2 \
  --replication-factor 1 \
  --config retention.ms=1209600000 \
  --bootstrap-server localhost:9092

echo "âœ… All topics created successfully!"
```

**Validation:**
```bash
chmod +x kafka-topics.sh
./kafka-topics.sh

# List all topics
docker exec millionx-kafka kafka-topics --list --bootstrap-server localhost:9092
```

#### Day 5: Monitoring Setup

**Install Prometheus + Grafana:**
```yaml
# Add to docker-compose.kafka.yml

  prometheus:
    image: prom/prometheus:latest
    container_name: millionx-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana:latest
    container_name: millionx-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  prometheus_data:
  grafana_data:
```

**prometheus.yml:**
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka:9101']
```

---

### Week 2: The "Eyes" - Scraper Agents

#### Day 1-3: Social Media Scrapers

**Project Structure:**
```
scrapers/
â”œâ”€â”€ social/
â”‚   â”œâ”€â”€ tiktok_scraper.py
â”‚   â”œâ”€â”€ facebook_scraper.py
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ market/
â”‚   â”œâ”€â”€ shopify_integration.py
â”‚   â””â”€â”€ daraz_integration.py
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”œâ”€â”€ dlq_handler.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

**models.py (Pydantic Schemas):**
```python
from pydantic import BaseModel, Field, validator
from datetime import datetime
from typing import Optional, List

class SocialPost(BaseModel):
    """Strict schema for social media posts"""
    platform: str = Field(..., regex="^(tiktok|facebook)$")
    post_id: str
    author_username: str  # Will be hashed by privacy shield
    content: str
    engagement_count: int = Field(ge=0)
    timestamp: datetime
    hashtags: List[str] = []
    product_mentions: Optional[List[str]] = None
    
    @validator('content')
    def content_not_empty(cls, v):
        if not v or len(v.strip()) < 5:
            raise ValueError('Content must be at least 5 characters')
        return v

class MarketOrder(BaseModel):
    """Schema for e-commerce orders"""
    platform: str = Field(..., regex="^(shopify|daraz)$")
    order_id: str
    merchant_id: str
    product_id: str
    product_name: str
    quantity: int = Field(gt=0)
    price: float = Field(gt=0)
    currency: str = Field(default="BDT")
    order_status: str
    customer_phone: Optional[str] = None  # Will be hashed
    timestamp: datetime

class WeatherData(BaseModel):
    """Schema for weather updates"""
    city: str
    temperature: float
    condition: str  # "clear", "rain", "storm", etc.
    humidity: float = Field(ge=0, le=100)
    wind_speed: float
    timestamp: datetime
    is_extreme_weather: bool = False
```

**tiktok_scraper.py:**
```python
import asyncio
from playwright.async_api import async_playwright
from kafka_producer import KafkaProducerClient
from models import SocialPost
from dlq_handler import send_to_dlq
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TikTokScraper:
    def __init__(self, keywords: List[str]):
        self.keywords = keywords
        self.producer = KafkaProducerClient(topic="source.social.tiktok")
        
    async def scrape(self):
        """Scrape TikTok for trending Benglish commerce keywords"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            for keyword in self.keywords:
                try:
                    # Navigate to TikTok search
                    url = f"https://www.tiktok.com/search?q={keyword}"
                    await page.goto(url, wait_until="networkidle")
                    
                    # Wait for posts to load
                    await page.wait_for_selector('[data-e2e="search-result"]', timeout=10000)
                    
                    # Extract posts
                    posts = await page.query_selector_all('[data-e2e="search-result"]')
                    
                    for post in posts[:20]:  # Limit to top 20
                        try:
                            # Extract data
                            author = await post.query_selector('[data-e2e="search-user-name"]')
                            content = await post.query_selector('[data-e2e="search-desc"]')
                            likes = await post.query_selector('[data-e2e="like-count"]')
                            
                            if not (author and content):
                                continue
                            
                            author_text = await author.inner_text()
                            content_text = await content.inner_text()
                            likes_text = await likes.inner_text() if likes else "0"
                            
                            # Validate with Pydantic
                            social_post = SocialPost(
                                platform="tiktok",
                                post_id=f"tiktok_{datetime.now().timestamp()}_{hash(content_text)}",
                                author_username=author_text,
                                content=content_text,
                                engagement_count=self._parse_count(likes_text),
                                timestamp=datetime.now(),
                                hashtags=[keyword],
                                product_mentions=self._extract_products(content_text)
                            )
                            
                            # Send to Kafka
                            await self.producer.send(social_post.dict())
                            logger.info(f"âœ… Scraped TikTok post: {social_post.post_id}")
                            
                        except Exception as e:
                            # Send to DLQ if validation fails
                            logger.error(f"âŒ Failed to process post: {e}")
                            await send_to_dlq(
                                topic="dead-letters-social",
                                data={"error": str(e), "raw": await post.inner_html()}
                            )
                    
                    # Rate limiting
                    await asyncio.sleep(5)
                    
                except Exception as e:
                    logger.error(f"âŒ Failed to scrape keyword '{keyword}': {e}")
            
            await browser.close()
    
    def _parse_count(self, text: str) -> int:
        """Parse TikTok count format (e.g., '1.2K' -> 1200)"""
        text = text.strip().upper()
        multipliers = {'K': 1000, 'M': 1000000}
        
        for suffix, mult in multipliers.items():
            if suffix in text:
                return int(float(text.replace(suffix, '')) * mult)
        return int(text) if text.isdigit() else 0
    
    def _extract_products(self, text: str) -> List[str]:
        """Extract product mentions from text"""
        # Simple keyword matching (improve with NER later)
        products = []
        product_keywords = ['bag', 'shoes', 'dress', 'phone', 'watch', 'juta', 'shari']
        
        for keyword in product_keywords:
            if keyword.lower() in text.lower():
                products.append(keyword)
        
        return products

# Run as Kubernetes CronJob
if __name__ == "__main__":
    keywords = [
        "online shopping bangladesh",
        "daraz deal",
        "facebook shop",
        "delivery dhaka",
        "trending product"
    ]
    
    scraper = TikTokScraper(keywords)
    asyncio.run(scraper.scrape())
```

**kafka_producer.py:**
```python
from kafka import KafkaProducer
import json
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class KafkaProducerClient:
    def __init__(self, topic: str, bootstrap_servers: str = "localhost:9092"):
        self.topic = topic
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',  # Wait for all replicas
            retries=3,
            max_in_flight_requests_per_connection=1  # Ensure ordering
        )
    
    async def send(self, data: Dict[str, Any]):
        """Send message to Kafka topic"""
        try:
            future = self.producer.send(self.topic, value=data)
            record_metadata = future.get(timeout=10)
            logger.info(f"âœ… Sent to {self.topic}: partition {record_metadata.partition}, offset {record_metadata.offset}")
        except Exception as e:
            logger.error(f"âŒ Failed to send to Kafka: {e}")
            raise
    
    def close(self):
        self.producer.flush()
        self.producer.close()
```

**dlq_handler.py:**
```python
from kafka_producer import KafkaProducerClient
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

async def send_to_dlq(topic: str, data: dict, error: str = None):
    """Send failed message to Dead Letter Queue"""
    dlq_producer = KafkaProducerClient(topic=topic)
    
    dlq_message = {
        "original_data": data,
        "error": error or "Unknown error",
        "timestamp": datetime.now().isoformat(),
        "retry_count": 0
    }
    
    try:
        await dlq_producer.send(dlq_message)
        logger.info(f"ðŸ“® Sent to DLQ: {topic}")
    except Exception as e:
        logger.error(f"âŒ Failed to send to DLQ: {e}")
    finally:
        dlq_producer.close()
```

**requirements.txt:**
```
kafka-python==2.0.2
playwright==1.40.0
pydantic==2.5.0
python-dotenv==1.0.0
requests==2.31.0
```

**Dockerfile:**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies for Playwright
RUN apt-get update && apt-get install -y \
    wget \
    ca-certificates \
    fonts-liberation \
    libasound2 \
    libatk-bridge2.0-0 \
    libatk1.0-0 \
    libc6 \
    libcairo2 \
    libcups2 \
    libdbus-1-3 \
    libexpat1 \
    libfontconfig1 \
    libgbm1 \
    libgcc1 \
    libglib2.0-0 \
    libgtk-3-0 \
    libnspr4 \
    libnss3 \
    libpango-1.0-0 \
    libpangocairo-1.0-0 \
    libstdc++6 \
    libx11-6 \
    libx11-xcb1 \
    libxcb1 \
    libxcomposite1 \
    libxcursor1 \
    libxdamage1 \
    libxext6 \
    libxfixes3 \
    libxi6 \
    libxrandr2 \
    libxrender1 \
    libxss1 \
    libxtst6 \
    lsb-release \
    xdg-utils \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install Playwright browsers
RUN playwright install chromium

COPY . .

CMD ["python", "social/tiktok_scraper.py"]
```

#### Day 4-5: Market Integrations

**shopify_integration.py:**
```python
import shopify
from kafka_producer import KafkaProducerClient
from models import MarketOrder
from datetime import datetime
import logging
import os

logger = logging.getLogger(__name__)

class ShopifyIntegration:
    def __init__(self):
        # Setup Shopify API
        shop_url = os.getenv("SHOPIFY_SHOP_URL")
        api_version = os.getenv("SHOPIFY_API_VERSION", "2024-01")
        access_token = os.getenv("SHOPIFY_ACCESS_TOKEN")
        
        shopify.ShopifyResource.set_site(f"https://{shop_url}/admin/api/{api_version}")
        shopify.ShopifyResource.activate_session(shopify.Session(shop_url, api_version, access_token))
        
        self.producer = KafkaProducerClient(topic="source.market.shopify")
    
    async def sync_orders(self, since_id: int = None):
        """Sync orders from Shopify"""
        try:
            # Fetch orders
            params = {"status": "any", "limit": 250}
            if since_id:
                params["since_id"] = since_id
            
            orders = shopify.Order.find(**params)
            
            for order in orders:
                try:
                    for line_item in order.line_items:
                        # Validate with Pydantic
                        market_order = MarketOrder(
                            platform="shopify",
                            order_id=str(order.id),
                            merchant_id=os.getenv("MERCHANT_ID"),
                            product_id=str(line_item.product_id),
                            product_name=line_item.name,
                            quantity=line_item.quantity,
                            price=float(line_item.price),
                            currency=order.currency,
                            order_status=order.financial_status,
                            customer_phone=order.customer.phone if order.customer else None,
                            timestamp=datetime.fromisoformat(order.created_at)
                        )
                        
                        # Send to Kafka
                        await self.producer.send(market_order.dict())
                        logger.info(f"âœ… Synced Shopify order: {order.id}")
                
                except Exception as e:
                    logger.error(f"âŒ Failed to process order {order.id}: {e}")
                    await send_to_dlq("dead-letters-market", order.to_dict(), str(e))
        
        except Exception as e:
            logger.error(f"âŒ Shopify sync failed: {e}")
    
    def setup_webhook(self):
        """Setup webhook for real-time order updates"""
        webhook = shopify.Webhook()
        webhook.topic = "orders/create"
        webhook.address = os.getenv("WEBHOOK_URL") + "/shopify/orders"
        webhook.format = "json"
        
        if webhook.save():
            logger.info("âœ… Shopify webhook created")
        else:
            logger.error(f"âŒ Webhook creation failed: {webhook.errors.full_messages()}")

if __name__ == "__main__":
    integration = ShopifyIntegration()
    asyncio.run(integration.sync_orders())
```

---

### Week 3: The "Brain" - Stream Processing

#### Day 1-2: Privacy Shield (PII Anonymization)

**privacy_shield.py:**
```python
import faust
import hashlib
import re
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

app = faust.App(
    'privacy-shield',
    broker='kafka://localhost:9092',
    value_serializer='json'
)

# Input topics
social_topic = app.topic('source.social.tiktok', 'source.social.facebook')
market_topic = app.topic('source.market.shopify', 'source.market.daraz')

# Output topics
anonymized_social = app.topic('sink.weaviate.vectors')
anonymized_market = app.topic('sink.snowflake.orders')

# Regex patterns for PII
PHONE_PATTERN = re.compile(r'\+?880[0-9]{10}|\b0[0-9]{10}\b')
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
NAME_PATTERN = re.compile(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b')  # Simple heuristic

SALT = "millionx_privacy_salt_2025"  # Move to secrets manager in prod

def hash_pii(text: str) -> str:
    """Hash PII with SHA-256"""
    return hashlib.sha256((text + SALT).encode()).hexdigest()[:16]

def anonymize_text(text: str) -> str:
    """Replace PII in text with hashed versions"""
    # Anonymize phone numbers
    text = PHONE_PATTERN.sub(lambda m: f"PHONE_{hash_pii(m.group())}", text)
    
    # Anonymize emails
    text = EMAIL_PATTERN.sub(lambda m: f"EMAIL_{hash_pii(m.group())}", text)
    
    # Anonymize potential names (be conservative)
    # text = NAME_PATTERN.sub(lambda m: f"USER_{hash_pii(m.group())}", text)
    
    return text

@app.agent(social_topic)
async def process_social(stream):
    """Anonymize social media posts"""
    async for message in stream:
        try:
            # Anonymize username
            if 'author_username' in message:
                message['author_username_hash'] = hash_pii(message['author_username'])
                del message['author_username']
            
            # Anonymize content
            if 'content' in message:
                message['content'] = anonymize_text(message['content'])
            
            # Forward to next stage
            await anonymized_social.send(value=message)
            logger.info(f"âœ… Anonymized social post: {message.get('post_id')}")
        
        except Exception as e:
            logger.error(f"âŒ Privacy shield failed: {e}")

@app.agent(market_topic)
async def process_market(stream):
    """Anonymize market orders"""
    async for message in stream:
        try:
            # Hash customer phone
            if message.get('customer_phone'):
                message['customer_phone_hash'] = hash_pii(message['customer_phone'])
                del message['customer_phone']
            
            # Forward to Snowflake
            await anonymized_market.send(value=message)
            logger.info(f"âœ… Anonymized order: {message.get('order_id')}")
        
        except Exception as e:
            logger.error(f"âŒ Privacy shield failed: {e}")

if __name__ == '__main__':
    app.main()
```

**Run:**
```bash
# Install Faust
pip install faust-streaming

# Start privacy shield worker
faust -A privacy_shield worker -l info
```

#### Day 3-4: Context Enrichment

**context_enricher.py:**
```python
import faust
import redis
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

app = faust.App(
    'context-enricher',
    broker='kafka://localhost:9092',
    value_serializer='json'
)

# Redis for product metadata cache
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

input_topic = app.topic('sink.weaviate.vectors')
output_topic = app.topic('enriched.weaviate.vectors')

@app.agent(input_topic)
async def enrich_social(stream):
    """Enrich social posts with product context"""
    async for message in stream:
        try:
            product_mentions = message.get('product_mentions', [])
            
            if product_mentions:
                # Look up product metadata from Redis cache
                enriched_context = []
                
                for product_id in product_mentions:
                    product_data = redis_client.get(f"product:{product_id}")
                    
                    if product_data:
                        enriched_context.append(product_data)
                    else:
                        logger.warning(f"âš ï¸ Product {product_id} not found in cache")
                
                # Create composite text for embedding
                message['enriched_content'] = f"{message.get('content', '')} [Products: {', '.join(enriched_context)}]"
            else:
                message['enriched_content'] = message.get('content', '')
            
            await output_topic.send(value=message)
            logger.info(f"âœ… Enriched post: {message.get('post_id')}")
        
        except Exception as e:
            logger.error(f"âŒ Enrichment failed: {e}")

if __name__ == '__main__':
    app.main()
```

#### Day 5: Embedding Service

**embedding_service.py:**
```python
import faust
from sentence_transformers import SentenceTransformer
import weaviate
from typing import Dict, Any
import logging
import numpy as np

logger = logging.getLogger(__name__)

app = faust.App(
    'embedding-service',
    broker='kafka://localhost:9092',
    value_serializer='json'
)

# Load embedding model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Connect to Weaviate
weaviate_client = weaviate.Client("http://localhost:8082")

input_topic = app.topic('enriched.weaviate.vectors')

@app.agent(input_topic)
async def embed_and_store(stream):
    """Generate embeddings and store in Weaviate"""
    async for message in stream:
        try:
            # Get text to embed
            text = message.get('enriched_content', message.get('content', ''))
            
            if not text:
                logger.warning("âš ï¸ No content to embed")
                continue
            
            # Generate embedding
            vector = model.encode(text).tolist()
            
            # Prepare Weaviate object
            data_object = {
                "platform": message.get('platform'),
                "content": text,
                "timestamp": message.get('timestamp'),
                "engagement": message.get('engagement_count', 0),
                "post_id": message.get('post_id'),
                "hashtags": message.get('hashtags', [])
            }
            
            # Store in Weaviate
            result = weaviate_client.data_object.create(
                data_object=data_object,
                class_name="SocialPost",
                vector=vector
            )
            
            logger.info(f"âœ… Stored in Weaviate: {result}")
        
        except Exception as e:
            logger.error(f"âŒ Embedding failed: {e}")

if __name__ == '__main__':
    app.main()
```

---

### Week 4: Storage & Monitoring

#### Day 1-2: Snowflake Integration

**snowflake_sink.py:**
```python
from kafka import KafkaConsumer
import snowflake.connector
import json
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class SnowflakeSink:
    def __init__(self):
        self.conn = snowflake.connector.connect(
            user=os.getenv('SNOWFLAKE_USER'),
            password=os.getenv('SNOWFLAKE_PASSWORD'),
            account=os.getenv('SNOWFLAKE_ACCOUNT'),
            warehouse='COMPUTE_WH',
            database='MILLIONX',
            schema='RAW_DATA'
        )
        
        self.consumer = KafkaConsumer(
            'sink.snowflake.orders',
            bootstrap_servers='localhost:9092',
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id='snowflake-sink'
        )
    
    def consume_and_load(self):
        """Consume from Kafka and load to Snowflake"""
        cursor = self.conn.cursor()
        
        for message in self.consumer:
            try:
                data = message.value
                
                # Insert into Snowflake
                cursor.execute("""
                    INSERT INTO orders (
                        order_id, platform, merchant_id, product_id,
                        product_name, quantity, price, currency,
                        order_status, customer_phone_hash, timestamp
                    ) VALUES (
                        %(order_id)s, %(platform)s, %(merchant_id)s, %(product_id)s,
                        %(product_name)s, %(quantity)s, %(price)s, %(currency)s,
                        %(order_status)s, %(customer_phone_hash)s, %(timestamp)s
                    )
                """, data)
                
                self.conn.commit()
                logger.info(f"âœ… Loaded to Snowflake: {data['order_id']}")
            
            except Exception as e:
                logger.error(f"âŒ Snowflake load failed: {e}")
                self.conn.rollback()

if __name__ == '__main__':
    sink = SnowflakeSink()
    sink.consume_and_load()
```

**Snowflake Schema:**
```sql
-- Create database
CREATE DATABASE MILLIONX;

-- Create schema
CREATE SCHEMA RAW_DATA;

-- Orders table
CREATE TABLE orders (
    order_id VARCHAR(100) PRIMARY KEY,
    platform VARCHAR(50),
    merchant_id VARCHAR(100),
    product_id VARCHAR(100),
    product_name VARCHAR(500),
    quantity INTEGER,
    price FLOAT,
    currency VARCHAR(10),
    order_status VARCHAR(50),
    customer_phone_hash VARCHAR(64),
    timestamp TIMESTAMP,
    ingestion_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Create indexes
CREATE INDEX idx_merchant ON orders(merchant_id);
CREATE INDEX idx_timestamp ON orders(timestamp);
CREATE INDEX idx_product ON orders(product_id);

-- Price history table
CREATE TABLE price_history (
    id INTEGER AUTOINCREMENT PRIMARY KEY,
    product_id VARCHAR(100),
    platform VARCHAR(50),
    price FLOAT,
    recorded_at TIMESTAMP
);

-- Weather logs table
CREATE TABLE weather_logs (
    id INTEGER AUTOINCREMENT PRIMARY KEY,
    city VARCHAR(100),
    temperature FLOAT,
    condition VARCHAR(100),
    humidity FLOAT,
    wind_speed FLOAT,
    is_extreme_weather BOOLEAN,
    timestamp TIMESTAMP
);
```

#### Day 3: Weaviate Setup

**weaviate_schema.py:**
```python
import weaviate

client = weaviate.Client("http://localhost:8082")

# Define schema
schema = {
    "classes": [
        {
            "class": "SocialPost",
            "description": "Social media posts with semantic embeddings",
            "vectorizer": "none",  # We provide vectors manually
            "properties": [
                {
                    "name": "platform",
                    "dataType": ["string"],
                    "description": "Social media platform (tiktok/facebook)"
                },
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "Post content"
                },
                {
                    "name": "timestamp",
                    "dataType": ["date"],
                    "description": "Post timestamp"
                },
                {
                    "name": "engagement",
                    "dataType": ["int"],
                    "description": "Total engagement count"
                },
                {
                    "name": "post_id",
                    "dataType": ["string"],
                    "description": "Unique post identifier"
                },
                {
                    "name": "hashtags",
                    "dataType": ["string[]"],
                    "description": "Associated hashtags"
                }
            ]
        }
    ]
}

# Create schema
client.schema.create(schema)
print("âœ… Weaviate schema created")
```

**Deploy Weaviate:**
```yaml
# Add to docker-compose
  weaviate:
    image: semitechnologies/weaviate:1.23.0
    container_name: millionx-weaviate
    ports:
      - "8082:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate_data:/var/lib/weaviate
```

#### Day 4: Weather Integration

**weather_fetcher.py:**
```python
import requests
from kafka_producer import KafkaProducerClient
from models import WeatherData
from datetime import datetime
import logging
import os

logger = logging.getLogger(__name__)

class WeatherFetcher:
    def __init__(self):
        self.api_key = os.getenv("OPENWEATHER_API_KEY")
        self.cities = ["Dhaka", "Chittagong", "Sylhet", "Khulna", "Rajshahi"]
        self.producer = KafkaProducerClient(topic="context.weather")
    
    async def fetch_weather(self):
        """Fetch weather for all major cities"""
        for city in self.cities:
            try:
                url = f"https://api.openweathermap.org/data/2.5/weather?q={city},BD&appid={self.api_key}&units=metric"
                response = requests.get(url)
                response.raise_for_status()
                
                data = response.json()
                
                # Check for extreme weather
                is_extreme = (
                    data['main']['temp'] > 40 or
                    data['main']['temp'] < 5 or
                    data['wind']['speed'] > 15 or
                    'rain' in data.get('weather', [{}])[0].get('main', '').lower()
                )
                
                weather = WeatherData(
                    city=city,
                    temperature=data['main']['temp'],
                    condition=data['weather'][0]['main'],
                    humidity=data['main']['humidity'],
                    wind_speed=data['wind']['speed'],
                    timestamp=datetime.now(),
                    is_extreme_weather=is_extreme
                )
                
                await self.producer.send(weather.dict())
                logger.info(f"âœ… Fetched weather for {city}: {weather.condition}")
                
            except Exception as e:
                logger.error(f"âŒ Failed to fetch weather for {city}: {e}")

# Setup as Kubernetes CronJob (every hour)
if __name__ == "__main__":
    import asyncio
    fetcher = WeatherFetcher()
    asyncio.run(fetcher.fetch_weather())
```

#### Day 5: Monitoring Dashboard

**Key Metrics to Track:**
1. **Kafka Lag:** Consumer group lag per topic
2. **Throughput:** Messages/sec ingested
3. **DLQ Rate:** Failed messages %
4. **Scraper Success Rate:** Successful scrapes %
5. **Embedding Latency:** P95/P99 latency
6. **Storage Write Rate:** Snowflake/Weaviate inserts/sec

**Grafana Dashboard JSON:**
```json
{
  "dashboard": {
    "title": "MillionX Phase 2 - Data Pipeline",
    "panels": [
      {
        "title": "Kafka Consumer Lag",
        "targets": [
          {
            "expr": "kafka_consumer_lag{topic=~'source.*'}",
            "legendFormat": "{{topic}}"
          }
        ]
      },
      {
        "title": "DLQ Message Rate",
        "targets": [
          {
            "expr": "rate(kafka_topic_messages_total{topic=~'dead-letters.*'}[5m])",
            "legendFormat": "{{topic}}"
          }
        ]
      }
    ]
  }
}
```

---

## ðŸ§ª Testing Checklist

### Unit Tests
- [ ] Pydantic model validation (valid/invalid data)
- [ ] PII anonymization function (phone/email detection)
- [ ] Embedding generation (verify vector dimensions)
- [ ] DLQ handler (ensure messages reach DLQ)

### Integration Tests
- [ ] End-to-end flow: Scraper â†’ Kafka â†’ Weaviate
- [ ] Privacy shield: PII removed before storage
- [ ] Context enricher: Product metadata attached
- [ ] Snowflake sink: Data lands in correct tables

### Performance Tests
- [ ] Kafka throughput: >10,000 msg/sec
- [ ] Embedding latency: P95 < 100ms
- [ ] Scraper completion: <5 minutes for 100 posts
- [ ] DLQ rate: <2% of total messages

---

## ðŸš€ Deployment Guide

### Kubernetes CronJobs

**k8s/tiktok-scraper-cronjob.yaml:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: tiktok-scraper
spec:
  schedule: "0 * * * *"  # Every hour
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: scraper
            image: millionx/tiktok-scraper:latest
            env:
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "kafka:9092"
          restartPolicy: OnFailure
```

### Faust Workers (Kubernetes Deployment)

**k8s/privacy-shield-deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: privacy-shield
spec:
  replicas: 3
  selector:
    matchLabels:
      app: privacy-shield
  template:
    metadata:
      labels:
        app: privacy-shield
    spec:
      containers:
      - name: worker
        image: millionx/privacy-shield:latest
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka:9092"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

---

## ðŸ“Š Success Validation

### Week 1 âœ…
- Kafka cluster running
- All topics created
- Monitoring dashboards live

### Week 2 âœ…
- Scrapers fetching 100+ posts/hour
- DLQ capturing invalid messages
- Market integrations syncing orders

### Week 3 âœ…
- PII anonymized at source
- Context enrichment working
- Embeddings generated (<100ms P95)

### Week 4 âœ…
- Snowflake receiving structured data
- Weaviate storing vectors
- Weather data streaming hourly
- All metrics within targets

---

## ðŸ”„ Next Steps â†’ Phase 3

Once Phase 2 is validated, proceed to:
1. **AI Model Training:** Use Snowflake data for NeuralProphet demand forecasting
2. **LangGraph Integration:** Connect agents to data sources
3. **Real-time Analytics:** Build dashboards for merchants
4. **Predictive Alerts:** Notify merchants of trending products

**Estimated Start Date:** Week 5  
**Prerequisites:** Phase 2 pipeline running at >95% uptime for 1 week

---

**Document Version:** 1.0  
**Last Updated:** December 20, 2025  
**Next Review:** After Week 2 completion
